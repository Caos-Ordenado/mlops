{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Enhance WebCrawlerClient with crawl_single Method",
        "description": "Implement a crawl_single method in the shared WebCrawlerClient to support individual URL crawling with caching and error handling.",
        "details": "Extend the existing WebCrawlerClient class to add a new crawl_single method that:\n1. Accepts a single URL as input\n2. Implements a 5-minute TTL cache for crawled pages\n3. Handles network errors and timeouts gracefully\n4. Returns the full page content for a given URL\n\nImplementation pseudocode:\n```python\nclass WebCrawlerClient:\n    # Existing methods...\n    \n    async def crawl_single(self, url: str) -> CrawlResponse:\n        # Check cache first\n        cached_response = self._check_cache(url)\n        if cached_response:\n            return cached_response\n            \n        # Call crawl-single endpoint\n        try:\n            response = await self._make_request(\n                endpoint=\"/crawl-single\",\n                payload={\"url\": url, \"timeout\": 8000}\n            )\n            \n            # Store in cache (5min TTL)\n            self._update_cache(url, response, ttl=300)\n            return response\n        except Exception as e:\n            # Log error\n            logger.error(f\"Failed to crawl {url}: {str(e)}\")\n            # Return error response\n            return CrawlResponse(success=False, error=str(e), content=None)\n```",
        "testStrategy": "1. Unit test the crawl_single method with mocked HTTP responses\n2. Test cache hit/miss scenarios\n3. Test error handling with simulated network failures\n4. Integration test with actual crawl-single endpoint\n5. Verify 5-minute TTL cache behavior",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Implement LLM-based PriceExtractorAgent",
        "description": "Replace the placeholder PriceExtractorAgent with a functional implementation that uses the phi3 model to extract prices from product pages.",
        "details": "Create a PriceExtractorAgent class that:\n1. Uses the Ollama client with phi3 model (temperature 0.0)\n2. Crafts optimized prompts for Uruguay e-commerce price extraction\n3. Processes page content to extract price information\n4. Returns structured JSON output\n\nImplementation pseudocode:\n```python\nclass PriceExtractorAgent:\n    def __init__(self, ollama_client):\n        self.ollama_client = ollama_client\n        self.model = \"phi3\"\n        \n    async def extract_price(self, page_content: str, url: str) -> PriceExtractionResult:\n        prompt = self._create_price_extraction_prompt(page_content, url)\n        \n        response = await self.ollama_client.generate(\n            model=self.model,\n            prompt=prompt,\n            temperature=0.0,\n            max_tokens=500\n        )\n        \n        # Parse JSON response\n        try:\n            extraction_result = self._parse_llm_response(response.text)\n            return PriceExtractionResult(\n                success=True,\n                price=extraction_result.get(\"price\"),\n                currency=extraction_result.get(\"currency\"),\n                original_text=extraction_result.get(\"original_text\"),\n                confidence=extraction_result.get(\"confidence\", 0.0)\n            )\n        except Exception as e:\n            logger.error(f\"Failed to extract price: {str(e)}\")\n            return PriceExtractionResult(success=False, error=str(e))\n            \n    def _create_price_extraction_prompt(self, page_content: str, url: str) -> str:\n        # Craft prompt with Uruguay e-commerce context\n        return f\"\"\"Extract the product price from this Uruguay e-commerce page.\n        URL: {url}\n        \n        Look for price formats like:\n        - $1,250 (UYU)\n        - UYU 1.250\n        - US$ 45 (USD)\n        \n        Return a JSON object with:\n        - price: the numeric price value\n        - currency: 'UYU' or 'USD'\n        - original_text: the original price text found\n        - confidence: your confidence in the extraction (0.0-1.0)\n        \n        Page content:\n        {page_content}\n        \"\"\"\n```",
        "testStrategy": "1. Unit test with sample page content from Uruguay e-commerce sites\n2. Test with various price formats ($1,250, UYU 1.250, US$ 45)\n3. Test error handling with malformed content\n4. Verify JSON parsing logic\n5. Integration test with real product pages",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "Develop Price Processing and Normalization",
        "description": "Implement logic to parse extracted price strings into numeric values, normalize currencies, and handle price ranges and discounts.",
        "details": "Create a PriceProcessor class that:\n1. Parses extracted price strings into numeric values\n2. Normalizes currencies (converts USD to UYU if needed)\n3. Handles price ranges, discounts, and promotions\n4. Validates prices for reasonableness\n\nImplementation pseudocode:\n```python\nclass PriceProcessor:\n    def __init__(self, usd_to_uyu_rate: float = 40.0):  # Example exchange rate\n        self.usd_to_uyu_rate = usd_to_uyu_rate\n        \n    def process_price(self, extraction_result: PriceExtractionResult) -> ProcessedPrice:\n        if not extraction_result.success:\n            return ProcessedPrice(success=False, error=extraction_result.error)\n            \n        try:\n            # Parse numeric value\n            numeric_price = self._parse_numeric_price(extraction_result.price)\n            \n            # Normalize currency\n            normalized_price = self._normalize_currency(\n                price=numeric_price,\n                currency=extraction_result.currency\n            )\n            \n            # Validate price\n            if not self._is_valid_price(normalized_price):\n                return ProcessedPrice(\n                    success=False,\n                    error=\"Invalid price value\",\n                    original=extraction_result\n                )\n                \n            return ProcessedPrice(\n                success=True,\n                price_uyu=normalized_price,\n                original_currency=extraction_result.currency,\n                original_text=extraction_result.original_text,\n                confidence=extraction_result.confidence\n            )\n        except Exception as e:\n            return ProcessedPrice(success=False, error=str(e), original=extraction_result)\n            \n    def _parse_numeric_price(self, price_str: str) -> float:\n        # Handle various formats, remove currency symbols, commas, etc.\n        # Return numeric value\n        \n    def _normalize_currency(self, price: float, currency: str) -> float:\n        # Convert to UYU if needed\n        if currency.upper() == \"USD\":\n            return price * self.usd_to_uyu_rate\n        return price\n        \n    def _is_valid_price(self, price: float) -> bool:\n        # Check if price is reasonable (positive, not too high, etc.)\n        return price > 0 and price < 1000000  # Example validation\n```",
        "testStrategy": "1. Unit test with various price formats and currencies\n2. Test currency conversion logic\n3. Test edge cases (very low/high prices, zero prices)\n4. Test with real examples from Uruguay e-commerce sites\n5. Verify handling of malformed price strings",
        "priority": "high",
        "dependencies": [
          2
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Create ProductWithPrice Data Model",
        "description": "Design and implement a ProductWithPrice data model that extends the existing IdentifiedPageCandidate model with price information.",
        "details": "Create a new data model that:\n1. Extends or wraps the existing IdentifiedPageCandidate model\n2. Adds price metadata (numeric value, currency, original text, confidence)\n3. Maintains backward compatibility\n\nImplementation pseudocode:\n```python\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, List\n\nclass PriceMetadata(BaseModel):\n    price_uyu: float\n    original_currency: str\n    original_text: str\n    confidence: float = Field(default=0.0, ge=0.0, le=1.0)\n    extraction_success: bool = True\n    error: Optional[str] = None\n\nclass ProductWithPrice(BaseModel):\n    # Original IdentifiedPageCandidate fields\n    url: str\n    title: str\n    page_type: str  # Should be \"PRODUCT\"\n    relevance_score: float\n    # Additional fields\n    price_data: Optional[PriceMetadata] = None\n    \n    @classmethod\n    def from_identified_page(cls, page: IdentifiedPageCandidate, price_data: Optional[PriceMetadata] = None):\n        return cls(\n            url=page.url,\n            title=page.title,\n            page_type=page.page_type,\n            relevance_score=page.relevance_score,\n            price_data=price_data\n        )\n        \n    def to_dict(self) -> dict:\n        # Convert to dict for API response\n        result = {\n            \"url\": self.url,\n            \"title\": self.title,\n            \"page_type\": self.page_type,\n            \"relevance_score\": self.relevance_score,\n        }\n        \n        # Add price data if available\n        if self.price_data and self.price_data.extraction_success:\n            result[\"price\"] = self.price_data.price_uyu\n            result[\"currency\"] = \"UYU\"\n            result[\"original_price_text\"] = self.price_data.original_text\n            \n        return result\n```",
        "testStrategy": "1. Unit test model creation and validation\n2. Test conversion from IdentifiedPageCandidate\n3. Verify to_dict() method maintains backward compatibility\n4. Test with various price data scenarios (success, failure)\n5. Validate Pydantic model constraints",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "Implement Price Extraction Pipeline",
        "description": "Create a pipeline that processes PRODUCT pages, extracts prices using the PriceExtractorAgent, and processes the results.",
        "details": "Implement a PriceExtractionPipeline class that:\n1. Filters for PRODUCT type pages\n2. Uses WebCrawlerClient.crawl_single to fetch page content\n3. Extracts prices using PriceExtractorAgent\n4. Processes prices with PriceProcessor\n5. Creates ProductWithPrice objects\n\nImplementation pseudocode:\n```python\nclass PriceExtractionPipeline:\n    def __init__(self, web_crawler: WebCrawlerClient, price_extractor: PriceExtractorAgent, price_processor: PriceProcessor):\n        self.web_crawler = web_crawler\n        self.price_extractor = price_extractor\n        self.price_processor = price_processor\n        \n    async def process_pages(self, pages: List[IdentifiedPageCandidate]) -> List[ProductWithPrice]:\n        # Filter for PRODUCT pages\n        product_pages = [p for p in pages if p.page_type == \"PRODUCT\"]\n        \n        results = []\n        # Process each product page\n        for page in product_pages:\n            try:\n                # Fetch page content\n                crawl_result = await self.web_crawler.crawl_single(page.url)\n                if not crawl_result.success:\n                    # Handle crawl failure\n                    results.append(ProductWithPrice.from_identified_page(\n                        page,\n                        PriceMetadata(extraction_success=False, error=f\"Crawl failed: {crawl_result.error}\")\n                    ))\n                    continue\n                    \n                # Extract price\n                extraction_result = await self.price_extractor.extract_price(\n                    page_content=crawl_result.content,\n                    url=page.url\n                )\n                \n                # Process price\n                processed_price = self.price_processor.process_price(extraction_result)\n                \n                # Create price metadata\n                if processed_price.success:\n                    price_metadata = PriceMetadata(\n                        price_uyu=processed_price.price_uyu,\n                        original_currency=processed_price.original_currency,\n                        original_text=processed_price.original_text,\n                        confidence=processed_price.confidence,\n                        extraction_success=True\n                    )\n                else:\n                    price_metadata = PriceMetadata(\n                        extraction_success=False,\n                        error=processed_price.error\n                    )\n                    \n                # Create ProductWithPrice\n                results.append(ProductWithPrice.from_identified_page(page, price_metadata))\n            except Exception as e:\n                # Handle unexpected errors\n                logger.error(f\"Error processing {page.url}: {str(e)}\")\n                results.append(ProductWithPrice.from_identified_page(\n                    page,\n                    PriceMetadata(extraction_success=False, error=str(e))\n                ))\n                \n        return results\n```",
        "testStrategy": "1. Unit test with mocked dependencies\n2. Test error handling for each step (crawl, extract, process)\n3. Integration test with real product pages\n4. Test with various page types (PRODUCT, CATEGORY, OTHER)\n5. Verify error resilience with simulated failures",
        "priority": "high",
        "dependencies": [
          1,
          2,
          3,
          4
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Implement Results Sorting by Price",
        "description": "Develop functionality to sort product results by price (cheapest first) while handling cases where price extraction fails.",
        "details": "Create a ResultSorter class that:\n1. Sorts ProductWithPrice objects by price (ascending)\n2. Handles cases where price extraction fails\n3. Maintains original order for non-PRODUCT pages\n\nImplementation pseudocode:\n```python\nclass ResultSorter:\n    def sort_by_price(self, products: List[ProductWithPrice]) -> List[ProductWithPrice]:\n        # Separate products with and without prices\n        products_with_price = []\n        products_without_price = []\n        \n        for product in products:\n            if (product.price_data and \n                product.price_data.extraction_success and \n                product.page_type == \"PRODUCT\"):\n                products_with_price.append(product)\n            else:\n                products_without_price.append(product)\n                \n        # Sort products with price by price (ascending)\n        sorted_products = sorted(\n            products_with_price,\n            key=lambda p: p.price_data.price_uyu\n        )\n        \n        # Append products without price\n        sorted_products.extend(products_without_price)\n        \n        return sorted_products\n```",
        "testStrategy": "1. Unit test with various product combinations\n2. Test sorting logic with known price values\n3. Verify handling of products without prices\n4. Test with empty lists and edge cases\n5. Verify original order is maintained for non-PRODUCT pages",
        "priority": "medium",
        "dependencies": [
          4,
          5
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Integrate with Existing Agent Pipeline",
        "description": "Integrate the price extraction and sorting functionality with the existing product search agent pipeline.",
        "details": "Update the main agent pipeline to:\n1. Use the new PriceExtractionPipeline\n2. Sort results by price\n3. Maintain backward compatibility\n4. Handle errors gracefully\n\nImplementation pseudocode:\n```python\nclass ProductSearchAgent:\n    # Existing initialization...\n    \n    async def search(self, query: str) -> SearchResponse:\n        # Existing search logic...\n        # This gets us to the point where we have classified pages\n        \n        # Extract prices for product pages\n        products_with_price = await self.price_extraction_pipeline.process_pages(classified_pages)\n        \n        # Sort results by price\n        sorted_products = self.result_sorter.sort_by_price(products_with_price)\n        \n        # Convert to API response format\n        results = [product.to_dict() for product in sorted_products]\n        \n        return SearchResponse(\n            query=query,\n            results=results,\n            metadata={\n                \"total_results\": len(results),\n                \"price_extraction_success_rate\": self._calculate_success_rate(products_with_price)\n            }\n        )\n        \n    def _calculate_success_rate(self, products: List[ProductWithPrice]) -> float:\n        # Calculate price extraction success rate\n        product_pages = [p for p in products if p.page_type == \"PRODUCT\"]\n        if not product_pages:\n            return 0.0\n            \n        successful_extractions = sum(\n            1 for p in product_pages \n            if p.price_data and p.price_data.extraction_success\n        )\n        \n        return successful_extractions / len(product_pages)\n```",
        "testStrategy": "1. Integration test with the full search pipeline\n2. Test with real queries and Uruguay e-commerce sites\n3. Verify backward compatibility of API responses\n4. Test error handling and resilience\n5. Measure performance against 10-second response time requirement",
        "priority": "high",
        "dependencies": [
          5,
          6
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "Implement Error Handling and Logging",
        "description": "Enhance the system with comprehensive error handling, retry logic, and logging for debugging price extraction issues.",
        "details": "Implement error handling and logging that:\n1. Adds retry logic for failed crawl attempts\n2. Logs detailed information for debugging\n3. Provides fallback behavior when price extraction fails\n4. Monitors extraction success rates\n\nImplementation pseudocode:\n```python\nclass ErrorHandler:\n    def __init__(self, max_retries: int = 2, retry_delay: float = 1.0):\n        self.max_retries = max_retries\n        self.retry_delay = retry_delay\n        \n    async def with_retry(self, func, *args, **kwargs):\n        last_error = None\n        for attempt in range(self.max_retries + 1):\n            try:\n                return await func(*args, **kwargs)\n            except Exception as e:\n                last_error = e\n                if attempt < self.max_retries:\n                    logger.warning(f\"Attempt {attempt+1} failed, retrying in {self.retry_delay}s: {str(e)}\")\n                    await asyncio.sleep(self.retry_delay)\n                else:\n                    logger.error(f\"All {self.max_retries+1} attempts failed: {str(e)}\")\n        \n        raise last_error\n\n# Enhanced logging for PriceExtractionPipeline\nclass EnhancedPriceExtractionPipeline(PriceExtractionPipeline):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.error_handler = ErrorHandler()\n        self.extraction_stats = {\n            \"attempts\": 0,\n            \"successes\": 0,\n            \"failures\": 0,\n            \"errors\": {}\n        }\n        \n    async def process_pages(self, pages: List[IdentifiedPageCandidate]) -> List[ProductWithPrice]:\n        # Reset stats\n        self.extraction_stats = {\n            \"attempts\": 0,\n            \"successes\": 0,\n            \"failures\": 0,\n            \"errors\": {}\n        }\n        \n        # Filter for PRODUCT pages\n        product_pages = [p for p in pages if p.page_type == \"PRODUCT\"]\n        self.extraction_stats[\"attempts\"] = len(product_pages)\n        \n        results = []\n        # Process each product page with enhanced error handling\n        for page in product_pages:\n            try:\n                # Use retry logic for crawling\n                crawl_result = await self.error_handler.with_retry(\n                    self.web_crawler.crawl_single,\n                    page.url\n                )\n                \n                # Rest of processing logic...\n                # [Similar to original implementation but with enhanced logging]\n                \n                # Track success\n                if price_metadata.extraction_success:\n                    self.extraction_stats[\"successes\"] += 1\n                else:\n                    self.extraction_stats[\"failures\"] += 1\n                    error_type = \"unknown\"\n                    if price_metadata.error:\n                        error_type = price_metadata.error.split(\":\")[0]\n                    self.extraction_stats[\"errors\"][error_type] = self.extraction_stats[\"errors\"].get(error_type, 0) + 1\n                    \n            except Exception as e:\n                # Handle unexpected errors\n                error_type = type(e).__name__\n                self.extraction_stats[\"failures\"] += 1\n                self.extraction_stats[\"errors\"][error_type] = self.extraction_stats[\"errors\"].get(error_type, 0) + 1\n                \n                logger.error(f\"Error processing {page.url}: {str(e)}\")\n                results.append(ProductWithPrice.from_identified_page(\n                    page,\n                    PriceMetadata(extraction_success=False, error=str(e))\n                ))\n                \n        # Log extraction stats\n        success_rate = self.extraction_stats[\"successes\"] / self.extraction_stats[\"attempts\"] if self.extraction_stats[\"attempts\"] > 0 else 0\n        logger.info(f\"Price extraction complete. Success rate: {success_rate:.2%}\")\n        logger.info(f\"Extraction stats: {self.extraction_stats}\")\n                \n        return results\n```",
        "testStrategy": "1. Test retry logic with simulated failures\n2. Verify logging output for various error scenarios\n3. Test extraction stats calculation\n4. Integration test with real-world failure cases\n5. Verify error classification and reporting",
        "priority": "medium",
        "dependencies": [
          5,
          7
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 9,
        "title": "Fix Critical Production Issues in Price Extraction Pipeline",
        "description": "Address five critical production issues found during testing: geo-validation bug, LLM price parsing errors, failed extractions in response, missing PRODUCT pages, and lack of 404/crawl failure handling.",
        "details": "Implement fixes for the following critical issues:\n\n1. Geo-validation bug:\n   - Update URL validation logic to properly filter out Colombian domains\n   - Implement a more robust country detection mechanism using TLD and domain analysis\n   - Add a comprehensive blocklist of Colombian domain patterns\n   - Update the regex patterns in the URL validator to correctly identify and filter Colombian sites\n\n2. LLM price parsing errors:\n   - Fix the PriceProcessor to correctly handle international price formats:\n     - Properly parse decimal separators (both commas and periods)\n     - Handle thousand separators correctly ($13.000,00 should be 13000.00, not 13.0)\n     - Fix currency symbol detection and placement issues\n   - Implement additional validation rules to catch implausible price conversions\n   - Add unit tests for various international price formats\n\n3. Failed extractions in response:\n   - Modify the PriceExtractionPipeline to filter out failed extractions\n   - Update the ResultSorter to properly handle and exclude failed extractions\n   - Implement a confidence threshold for price extractions\n   - Add logging for failed extractions for monitoring purposes\n\n4. Missing PRODUCT pages:\n   - Debug and fix the page classification logic to correctly identify all PRODUCT pages\n   - Review and update the extraction pipeline to ensure all PRODUCT pages are processed\n   - Implement additional heuristics to identify product pages that may be misclassified\n   - Add monitoring for page type classification accuracy\n\n5. 404/crawl failure handling:\n   - Integrate with the existing ErrorHandler to properly handle 404 responses\n   - Implement specific handling for crawl failures with appropriate status codes\n   - Add retry logic with exponential backoff for transient failures\n   - Ensure failed crawls don't cause the entire pipeline to fail\n\nImplementation approach:\n```python\n# Example fix for price parsing errors\ndef fix_price_parsing(price_text: str) -> float:\n    # Remove currency symbols and whitespace\n    cleaned = re.sub(r'[$€£¥]|\\s', '', price_text)\n    \n    # Handle different international formats\n    if ',' in cleaned and '.' in cleaned:\n        # Format like $13.000,00\n        if cleaned.find('.') < cleaned.find(','):\n            # Remove thousand separators and convert decimal comma to point\n            cleaned = cleaned.replace('.', '').replace(',', '.')\n        else:\n            # Format like $13,000.00\n            cleaned = cleaned.replace(',', '')\n    elif ',' in cleaned:\n        # Format like 13,00\n        cleaned = cleaned.replace(',', '.')\n    \n    try:\n        return float(cleaned)\n    except ValueError:\n        # Log the error and return None\n        logging.error(f\"Failed to parse price: {price_text}\")\n        return None\n\n# Example fix for filtering failed extractions\ndef filter_valid_extractions(extraction_results):\n    return [\n        result for result in extraction_results \n        if result.extraction_success and result.price_uyu is not None and result.price_uyu > 0\n    ]\n```",
        "testStrategy": "1. Geo-validation testing:\n   - Create a test suite with URLs from various countries, especially Colombia\n   - Verify Colombian URLs are properly filtered out\n   - Test edge cases like Colombian businesses with non-Colombian TLDs\n   - Perform integration tests with the full pipeline\n\n2. Price parsing testing:\n   - Create a comprehensive test suite with various international price formats\n   - Test specific examples from the bug report ($13.000,00 → 13000.00, $189 → 189.00)\n   - Test edge cases like very large/small numbers, different currencies\n   - Verify correct handling of thousand separators and decimal points\n   - Benchmark against a dataset of real-world prices from Uruguay\n\n3. Failed extraction filtering:\n   - Test that failed extractions are properly excluded from API responses\n   - Verify that extraction statistics are still accurately tracked\n   - Test with a mix of successful and failed extractions\n   - Verify logging is working correctly for failed extractions\n\n4. PRODUCT page testing:\n   - Create test cases with various page types including edge cases\n   - Verify all PRODUCT pages are correctly identified and processed\n   - Test with real-world examples of previously missed product pages\n   - Measure classification accuracy improvements\n\n5. 404/crawl failure testing:\n   - Simulate various HTTP errors (404, 500, timeout, etc.)\n   - Verify appropriate error handling for each case\n   - Test retry logic with mock services that fail intermittently\n   - Verify the pipeline continues processing other URLs when one fails\n   - Test end-to-end with intentionally invalid URLs\n\n6. Regression testing:\n   - Run the full test suite to ensure no new issues are introduced\n   - Verify all fixed issues remain fixed with appropriate test cases\n   - Perform load testing to ensure performance remains acceptable",
        "status": "done",
        "dependencies": [
          3,
          5,
          8
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 10,
        "title": "Optimize Price Extraction Pipeline for Better Coverage and Accuracy",
        "description": "Enhance the price extraction pipeline to address four key issues: incorrect LLM price parsing, overly restrictive content threshold, insufficient PRODUCT page classification, and limited search query coverage.",
        "details": "Implement the following improvements to the price extraction pipeline:\n\n1. Fix LLM price parsing issues:\n   - Update the prompt template in PriceExtractorAgent to provide clearer instructions about currency formats\n   - Add specific examples of correct price parsing for different formats ($189 → 189.00, not 45.50)\n   - Implement additional validation rules in PriceProcessor to catch and correct common parsing errors\n   - Add a post-processing step to verify extracted prices against expected ranges and patterns\n\n2. Adjust content threshold for processing:\n   - Lower the minimum character threshold from 51 to a more appropriate value (suggest 25-30 characters)\n   - Implement a more nuanced content filtering approach that considers:\n     - Presence of price-related keywords rather than just character count\n     - HTML structure indicators of product pages (price elements, add-to-cart buttons)\n     - Proportion of meaningful content vs. boilerplate text\n\n3. Improve PRODUCT page classification:\n   - Review and enhance the page classification logic in the pipeline\n   - Add additional classification features based on:\n     - URL patterns typical of product pages (/product/, /item/, /p/, etc.)\n     - HTML structure analysis (product schema markup, pricing elements)\n     - Content keyword density for product-related terms\n   - Implement a confidence score for classification to allow processing borderline cases\n   - Add logging for classification decisions to identify patterns for improvement\n\n4. Expand search query generation:\n   - Implement a strategy to generate more diverse search queries:\n     - Create a taxonomy of product categories relevant to the target market\n     - Generate category-specific query templates\n     - Incorporate popular brand names and product types from the region\n     - Use a combination of specific and generic product terms\n   - Implement query expansion techniques to create variations of successful queries\n   - Track query performance metrics to identify which query patterns yield the best product pages\n\nImplementation pseudocode for key components:\n\n```python\n# Enhanced LLM prompt for price extraction\nPRICE_EXTRACTION_PROMPT = \"\"\"\nExtract the exact price from this product page.\nIMPORTANT: Return the price as a decimal number only.\nExamples:\n- If you see \"$189.99\", return \"189.99\"\n- If you see \"$189\", return \"189.00\"\n- If you see \"189,99 €\", return \"189.99\"\n- DO NOT convert between currencies or modify the value in any way\n\nProduct page content:\n{content}\n\"\"\"\n\n# Improved content threshold logic\ndef should_process_content(content: str) -> bool:\n    # Lower minimum character threshold\n    if len(content) < 30:\n        return False\n        \n    # Check for price indicators even in shorter content\n    price_indicators = [\"$\", \"price\", \"USD\", \"cost\", \"valor\"]\n    if any(indicator in content for indicator in price_indicators):\n        return True\n        \n    # Additional heuristics for product page detection\n    return has_product_indicators(content)\n\n# Enhanced product classification\ndef classify_page(url: str, content: str) -> Tuple[PageType, float]:\n    # Existing classification logic\n    base_classification = existing_classifier(url, content)\n    \n    # Enhanced product page detection\n    product_confidence = 0.0\n    \n    # URL pattern matching\n    product_url_patterns = [\"/product/\", \"/item/\", \"/p/\", \"/buy/\"]\n    if any(pattern in url for pattern in product_url_patterns):\n        product_confidence += 0.3\n    \n    # Content analysis\n    if has_product_schema_markup(content):\n        product_confidence += 0.4\n    \n    if has_pricing_elements(content):\n        product_confidence += 0.3\n    \n    # Override classification if confidence is high enough\n    if product_confidence > 0.5 and base_classification != PageType.PRODUCT:\n        return PageType.PRODUCT, product_confidence\n    \n    return base_classification, product_confidence\n\n# Query expansion strategy\nclass QueryGenerator:\n    def __init__(self, base_queries: List[str], product_categories: List[str], brands: List[str]):\n        self.base_queries = base_queries\n        self.product_categories = product_categories\n        self.brands = brands\n    \n    def generate_expanded_queries(self) -> List[str]:\n        expanded_queries = []\n        \n        for base_query in self.base_queries:\n            # Add the base query\n            expanded_queries.append(base_query)\n            \n            # Add category-specific variations\n            for category in self.product_categories:\n                expanded_queries.append(f\"{base_query} {category}\")\n                \n            # Add brand-specific variations\n            for brand in self.brands:\n                expanded_queries.append(f\"{brand} {base_query}\")\n                \n            # Add combined variations\n            for category in self.product_categories:\n                for brand in self.brands:\n                    expanded_queries.append(f\"{brand} {category} {base_query}\")\n        \n        return expanded_queries\n```",
        "testStrategy": "1. LLM price parsing testing:\n   - Create a test suite with various price formats ($189, $189.99, 189,99 €, etc.)\n   - Verify the LLM correctly parses each format without conversion errors\n   - Test edge cases like prices with thousands separators ($1,189.99)\n   - Compare extraction accuracy before and after prompt improvements\n   - Implement automated regression tests to prevent future parsing issues\n\n2. Content threshold testing:\n   - Create a dataset of product pages with varying content lengths\n   - Test the new threshold against known product pages that were previously rejected\n   - Measure false positive and false negative rates with the new threshold\n   - Verify that legitimate product pages with minimal text are now correctly processed\n   - Test edge cases like pages with prices but minimal other content\n\n3. PRODUCT classification testing:\n   - Create a labeled dataset of PRODUCT and non-PRODUCT pages\n   - Measure classification accuracy, precision, and recall before and after changes\n   - Test with URLs from different e-commerce sites in the target region\n   - Verify improvement in the number of correctly classified PRODUCT pages\n   - Analyze misclassifications to identify further improvement opportunities\n\n4. Query generation testing:\n   - Measure the diversity and coverage of generated queries\n   - Test query performance by tracking:\n     - Number of PRODUCT pages discovered per query\n     - Success rate of price extraction for pages found by each query\n     - Overall increase in product coverage\n   - Compare results between original and expanded query sets\n   - Verify that new queries discover products in previously uncovered categories\n\n5. End-to-end pipeline testing:\n   - Run the complete pipeline with all improvements\n   - Measure key metrics before and after changes:\n     - Total number of PRODUCT pages processed\n     - Successful price extraction rate\n     - Accuracy of extracted prices\n     - Coverage across different product categories\n   - Perform A/B testing in production with a subset of traffic to validate improvements",
        "status": "done",
        "dependencies": [
          5,
          8,
          9
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 11,
        "title": "Debug Category Page Classification Issues",
        "description": "Investigate and fix the issue where catalog pages (like eldorado.com.uy/harinas, elnaranjo.com.uy/harinas-y-salvados) are being incorrectly classified as \"OTHER\" instead of \"CATEGORY\", preventing them from reaching the price extraction pipeline.",
        "status": "done",
        "dependencies": [
          5,
          9,
          10
        ],
        "priority": "high",
        "details": "This task involves debugging and fixing the page classification logic to correctly identify catalog/category pages. Root cause analysis has identified that poor search snippets are causing catalog pages to be misclassified as \"OTHER\" instead of \"CATEGORY\".\n\n1. Analyze the current page classification criteria (COMPLETED):\n   - Review of classification rules in the page classifier component revealed overly restrictive patterns\n   - URL patterns for category pages are not comprehensive enough\n   - Current heuristics miss common catalog URL patterns like /productos/, /almacen/, /comestibles/\n\n2. Collect and analyze problematic URLs (COMPLETED):\n   - Test dataset created with known category pages being misclassified\n   - Analysis shows clear catalog patterns in URLs that should be recognized\n   - Examples include eldorado.com.uy/harinas and elnaranjo.com.uy/productos/harinas-y-salvados\n\n3. Implement fixes based on root cause analysis:\n   - Update the URL pattern recognition to include additional common category URL patterns\n   - Add specific patterns identified: /productos/, /almacen/, /comestibles/\n   - Enhance content-based classification to better detect product grids and category indicators\n   - Add specific rules for problematic domains if necessary\n\n4. Update the PageClassifier implementation:\n   ```python\n   class PageClassifier:\n       def classify(self, url: str, content: str) -> PageType:\n           # Update URL pattern matching for category pages\n           if self._is_category_url(url) or self._has_category_content(content):\n               return PageType.CATEGORY\n           \n           # Rest of the classification logic...\n   \n       def _is_category_url(self, url: str) -> bool:\n           # Enhanced pattern matching for category URLs\n           category_patterns = [\n               r'/categor(y|ies|ia|ias)/',\n               r'/collections?/',\n               r'/productos?/',\n               r'/almacen/',\n               r'/comestibles/',\n               r'/([\\w-]+)/([\\w-]+)$',  # Common pattern for category/subcategory\n               r'/(harinas|bebidas|lacteos|[\\w-]+)/?$'  # Common product categories\n           ]\n           # Implementation of pattern matching\n   ```\n\n5. Add logging to track classification decisions:\n   - Log the signals that led to each classification decision\n   - Track classification statistics to monitor improvements\n\n6. Update the pipeline to ensure category pages are properly processed after classification.\n\n7. Ensure industry standard is followed by correctly classifying catalog pages as \"CATEGORY\" rather than processing \"OTHER\" pages.",
        "testStrategy": "1. Create a comprehensive test suite for page classification:\n   - Compile a dataset of at least 30 known category pages from various Uruguay e-commerce sites\n   - Include the specific problematic examples (eldorado.com.uy/harinas, elnaranjo.com.uy/harinas-y-salvados)\n   - Add examples with the newly identified patterns (/productos/, /almacen/, /comestibles/)\n   - Add a variety of other page types (product, home, search results) as control cases\n\n2. Implement automated classification testing:\n   - Create unit tests that verify each category page is correctly classified\n   - Test with both URL-only and full content classification\n   - Measure classification accuracy before and after the fix\n   - Specifically test URLs with the patterns identified in the root cause analysis\n\n3. Perform regression testing:\n   - Ensure that previously correctly classified pages still work\n   - Verify that product pages are still correctly identified\n   - Check that other page types aren't negatively affected\n\n4. End-to-end pipeline testing:\n   - Run the full pipeline with the updated classifier\n   - Verify that category pages now flow through to the appropriate processing steps\n   - Confirm that category pages appear correctly in search results\n\n5. Manual verification:\n   - Manually check the classification of the problematic URLs\n   - Review logs to confirm the classification logic is working as expected\n   - Verify the signals that led to the classification decision\n\n6. Monitor production metrics:\n   - Track the percentage of pages classified as CATEGORY vs OTHER\n   - Monitor any changes in overall pipeline performance\n   - Set up alerts for unexpected classification distribution changes",
        "subtasks": [
          {
            "id": 1,
            "title": "Analyze current page classification criteria",
            "description": "Review classification rules, URL pattern evaluation, and heuristics in the page classifier component",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Collect and analyze problematic URLs",
            "description": "Create test dataset of misclassified category pages and analyze their HTML structure",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Update URL pattern recognition",
            "description": "Add the identified catalog patterns (/productos/, /almacen/, /comestibles/) to the category URL patterns list",
            "status": "to-do",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Enhance content-based classification",
            "description": "Improve detection of product grids and category indicators in page content",
            "status": "to-do",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement logging for classification decisions",
            "description": "Add detailed logging to track signals that lead to classification decisions",
            "status": "to-do",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Create comprehensive test suite",
            "description": "Develop tests for all identified catalog patterns including the problematic examples",
            "status": "to-do",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Verify pipeline integration",
            "description": "Ensure category pages are properly processed through the entire pipeline after classification",
            "status": "to-do",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 12,
        "title": "Verify Content-Based Catalog Detection in Price Extraction Pipeline",
        "description": "Investigate and ensure that the content-based catalog detection improvements in price_extractor.py are being properly applied and not bypassed by early filtering stages in the pipeline.",
        "details": "This task involves verifying that content-based catalog detection logic in price_extractor.py is functioning as intended throughout the pipeline:\n\n1. Code Review:\n   - Examine the current implementation of price_extractor.py to understand the content-based catalog detection logic\n   - Review the pipeline flow to identify potential early filtering stages that might bypass this logic\n   - Map out the execution path from URL ingestion to price extraction, focusing on page classification steps\n\n2. Identify Potential Bypass Points:\n   - Check for early URL-based filtering that might reject catalog pages before content analysis\n   - Examine page type classification logic that occurs before price_extractor.py is invoked\n   - Look for hardcoded rules or patterns that might override content-based detection\n\n3. Implement Diagnostic Logging:\n   - Add detailed logging at key decision points in the pipeline\n   - Track the classification journey of catalog pages through the system\n   - Log both URL-based and content-based classification decisions for comparison\n\n4. Fix Bypass Issues:\n   - Modify any early filtering logic that incorrectly rejects catalog pages\n   - Ensure content-based detection has appropriate priority in classification decisions\n   - Update pipeline flow to properly leverage the content-based detection improvements\n\n5. Refactor Pipeline Order if Necessary:\n   - If early filtering cannot be modified, consider restructuring the pipeline order\n   - Implement a two-pass approach where content analysis can override initial URL-based classification\n   - Ensure content-based detection is applied to all potential catalog pages\n\nImplementation pseudocode:\n```python\n# Add diagnostic logging to track page classification\ndef classify_page(url, content):\n    # Log initial classification attempt\n    initial_classification = url_based_classification(url)\n    logging.debug(f\"Initial URL-based classification for {url}: {initial_classification}\")\n    \n    # Apply content-based detection\n    content_classification = content_based_classification(content)\n    logging.debug(f\"Content-based classification for {url}: {content_classification}\")\n    \n    # Determine final classification with proper priority\n    final_classification = determine_final_classification(initial_classification, content_classification)\n    logging.info(f\"Final classification for {url}: {final_classification}\")\n    \n    return final_classification\n\n# Ensure content-based detection is properly prioritized\ndef determine_final_classification(url_classification, content_classification):\n    # If content analysis strongly indicates CATEGORY, prioritize it\n    if content_classification == \"CATEGORY\" and content_classification_confidence > THRESHOLD:\n        return \"CATEGORY\"\n    \n    # Otherwise use existing logic\n    return existing_classification_logic(url_classification, content_classification)\n```",
        "testStrategy": "1. Create a Test Dataset:\n   - Compile a set of at least 20 known catalog pages that were previously misclassified\n   - Include pages from eldorado.com.uy/harinas, elnaranjo.com.uy/harinas-y-salvados and similar patterns\n   - Add examples with varying content structures and URL patterns\n\n2. Implement Pipeline Tracing:\n   - Create a test harness that logs the classification decision at each stage\n   - Track how each test URL progresses through the pipeline\n   - Record both the initial and final classification for each page\n\n3. A/B Testing:\n   - Process the test dataset through both the original and modified pipeline\n   - Compare classification results before and after changes\n   - Measure improvement in catalog page detection rate\n\n4. End-to-End Testing:\n   - Verify that correctly classified catalog pages proceed to the appropriate processing\n   - Confirm that prices are being extracted from newly detected catalog pages\n   - Validate that the entire pipeline functions correctly with the modified classification logic\n\n5. Regression Testing:\n   - Ensure that pages previously classified correctly are still handled properly\n   - Verify that product pages are still identified correctly\n   - Check that overall system performance is maintained or improved\n\n6. Monitoring Implementation:\n   - Add permanent logging to track classification decisions in production\n   - Create metrics to measure catalog detection rates over time\n   - Implement alerts for unexpected drops in catalog page detection",
        "status": "pending",
        "dependencies": [
          5,
          10,
          11
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 13,
        "title": "Fix Pipeline Filtering to Allow \"OTHER\" Pages to Reach Price Extraction",
        "description": "Modify the price extraction pipeline to ensure catalog pages initially classified as \"OTHER\" can still reach the price extraction step where content-based detection can reclassify them.",
        "details": "This task involves modifying the filtering logic in the PriceExtractionPipeline to allow pages initially classified as \"OTHER\" to be processed:\n\n1. Identify the current filtering logic in PriceExtractionPipeline:\n   - Locate the code that filters pages based on their classification type\n   - Currently, the pipeline likely only processes pages classified as \"PRODUCT\"\n   - Review how page type filtering is implemented before the price extraction step\n\n2. Modify the filtering logic:\n   - Update the filter to include both \"PRODUCT\" and \"OTHER\" page types\n   - Ensure \"CATEGORY\" pages continue to be processed as they currently are\n   - Maintain any existing optimization logic that prevents unnecessary processing\n\n3. Update the content-based detection flow:\n   - Ensure the content-based detection in price_extractor.py is applied to \"OTHER\" pages\n   - Add logic to reclassify pages based on content analysis\n   - Implement a confidence threshold for reclassification\n\n4. Implementation pseudocode:\n```python\nclass PriceExtractionPipeline:\n    # Existing initialization...\n    \n    async def process_pages(self, pages: List[IdentifiedPageCandidate]) -> List[ProductWithPrice]:\n        # Modified filtering logic\n        # Previously: product_pages = [p for p in pages if p.page_type == \"PRODUCT\"]\n        # Now include OTHER pages:\n        candidate_pages = [p for p in pages if p.page_type in [\"PRODUCT\", \"OTHER\"]]\n        \n        results = []\n        for page in candidate_pages:\n            # Fetch content\n            content = await self.web_crawler.crawl_single(page.url)\n            \n            # Extract prices\n            extraction_result = await self.price_extractor.extract_price(content, page.url)\n            \n            # If page was OTHER but content suggests it's a product, reclassify\n            if page.page_type == \"OTHER\" and extraction_result.content_suggests_product:\n                page.page_type = \"PRODUCT\"\n                page.classification_confidence = extraction_result.reclassification_confidence\n            \n            # Only process actual products\n            if page.page_type == \"PRODUCT\":\n                processed_price = self.price_processor.process_price(extraction_result)\n                results.append(ProductWithPrice.from_page_candidate(page, processed_price))\n            \n        return results\n```\n\n5. Update related components:\n   - Modify the PriceExtractorAgent to include a content_suggests_product flag in its response\n   - Add reclassification_confidence score to indicate certainty of reclassification\n   - Update any dependent components that might assume only PRODUCT pages are processed",
        "testStrategy": "1. Create a test dataset:\n   - Compile at least 15 known catalog pages that are currently being classified as \"OTHER\"\n   - Include the problematic examples mentioned in Task 11 (eldorado.com.uy/harinas, elnaranjo.com.uy/harinas-y-salvados)\n   - Add a variety of other misclassified pages from different Uruguay e-commerce sites\n\n2. Unit test the modified filtering logic:\n   - Verify that pages with \"OTHER\" classification are now included in the processing\n   - Test that the reclassification logic correctly identifies product pages\n   - Ensure that genuinely non-product \"OTHER\" pages don't produce false positives\n\n3. Integration testing:\n   - Run the full pipeline with the test dataset\n   - Verify that previously missed catalog pages now reach the price extraction step\n   - Confirm that content-based detection correctly reclassifies appropriate pages\n   - Check that the final results include products from pages initially classified as \"OTHER\"\n\n4. Regression testing:\n   - Ensure existing functionality for \"PRODUCT\" and \"CATEGORY\" pages remains unchanged\n   - Verify that performance is not significantly degraded by processing additional pages\n   - Check that the pipeline still handles errors gracefully\n\n5. End-to-end testing:\n   - Test with real user queries that previously missed products on catalog pages\n   - Verify improved coverage across the problematic sites\n   - Measure and document the improvement in product discovery rate",
        "status": "pending",
        "dependencies": [
          5,
          10,
          11,
          12
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 14,
        "title": "Implement Catalog Price Extraction API Testing",
        "description": "Create a comprehensive test suite to verify that the price extraction pipeline correctly processes catalog pages from sites like eldorado.com.uy and elnaranjo.com.uy, ensuring multiple products with prices are returned.",
        "details": "This task involves creating a dedicated test suite to verify that catalog pages are correctly processed by the price extraction pipeline:\n\n1. Set up a test environment:\n   - Create a new test module `tests/test_catalog_price_extraction.py`\n   - Import necessary dependencies (PriceExtractionPipeline, WebCrawlerClient, etc.)\n   - Set up test fixtures for the required components\n\n2. Create a collection of test catalog URLs:\n   - Compile at least 10 catalog URLs from eldorado.com.uy (e.g., /harinas, /lacteos, /bebidas)\n   - Compile at least 10 catalog URLs from elnaranjo.com.uy (e.g., /harinas-y-salvados, /lacteos-y-huevos)\n   - Include additional catalog URLs from other Uruguayan e-commerce sites for diversity\n\n3. Implement direct API testing:\n   - Create a test function that makes direct API requests to the price extraction endpoint\n   - Pass each catalog URL and verify the response structure\n   - Assert that multiple products are returned (at least 3 per catalog page)\n   - Verify each product has a valid price (non-null, positive value)\n\n4. Implement component-level testing:\n   - Create tests that directly use the PriceExtractionPipeline class\n   - Mock the WebCrawlerClient to return real HTML content from catalog pages\n   - Verify the pipeline correctly processes these pages and extracts multiple products with prices\n\n5. Test edge cases:\n   - Test catalog pages with mixed content (some products with prices, some without)\n   - Test catalog pages with pagination\n   - Test catalog pages with filters applied\n   - Test catalog pages with special promotions or discounts\n\n6. Create a reporting mechanism:\n   - Log detailed information about each test case\n   - Generate a summary report showing success rates for different sites\n   - Track the average number of products extracted per catalog page\n\n7. Implement continuous monitoring:\n   - Create a script that can be run periodically to verify catalog extraction remains functional\n   - Set up alerts for any significant drops in extraction performance\n\nSample test code:\n```python\nimport pytest\nimport aiohttp\nimport asyncio\nfrom app.price_extraction import PriceExtractionPipeline\nfrom app.web_crawler import WebCrawlerClient\n\nCATALOG_URLS = [\n    \"https://www.eldorado.com.uy/harinas\",\n    \"https://www.eldorado.com.uy/lacteos\",\n    \"https://www.elnaranjo.com.uy/harinas-y-salvados\",\n    \"https://www.elnaranjo.com.uy/lacteos-y-huevos\",\n    # Add more catalog URLs here\n]\n\n@pytest.mark.asyncio\nasync def test_catalog_price_extraction_api():\n    \"\"\"Test that catalog pages return multiple products with prices via API.\"\"\"\n    async with aiohttp.ClientSession() as session:\n        for url in CATALOG_URLS:\n            response = await session.post(\n                \"http://api-endpoint/extract-prices\",\n                json={\"url\": url}\n            )\n            data = await response.json()\n            \n            # Verify response structure\n            assert \"products\" in data\n            products = data[\"products\"]\n            \n            # Verify multiple products are returned\n            assert len(products) >= 3, f\"Expected at least 3 products for {url}, got {len(products)}\"\n            \n            # Verify each product has a valid price\n            products_with_price = [p for p in products if p.get(\"price_uyu\") is not None and p.get(\"price_uyu\") > 0]\n            assert len(products_with_price) >= 3, f\"Expected at least 3 products with valid prices for {url}\"\n            \n            # Log results\n            print(f\"URL: {url}, Total products: {len(products)}, Products with price: {len(products_with_price)}\")\n```",
        "testStrategy": "1. Prepare test data:\n   - Create a spreadsheet with at least 20 catalog URLs from different Uruguayan e-commerce sites\n   - For each URL, manually verify it's a catalog page that should contain multiple products with prices\n   - Document the expected minimum number of products for each URL\n\n2. API endpoint testing:\n   - Send HTTP requests to the price extraction API endpoint with each catalog URL\n   - Verify the HTTP response status code is 200\n   - Validate the JSON response structure matches the expected schema\n   - Assert that multiple products (at least 3) are returned for each catalog URL\n   - Verify each product has a valid price (non-null, positive value)\n   - Calculate and report the success rate (% of catalog pages returning expected products with prices)\n\n3. Component testing:\n   - Test the PriceExtractionPipeline directly with the catalog URLs\n   - Verify it correctly processes catalog pages and extracts products with prices\n   - Compare the results with the API endpoint testing to ensure consistency\n\n4. Performance testing:\n   - Measure the response time for each catalog URL\n   - Ensure the average response time is within acceptable limits (under 15 seconds)\n   - Test with concurrent requests to simulate real-world usage\n\n5. Regression testing:\n   - Compare results with previous test runs to identify any regressions\n   - Document any changes in extraction performance over time\n\n6. Error handling testing:\n   - Test with invalid URLs, malformed URLs, and URLs that return errors\n   - Verify the system handles these cases gracefully and returns appropriate error messages\n\n7. Create a comprehensive test report:\n   - Document the test results for each catalog URL\n   - Calculate overall success metrics (% of catalog pages with successful extraction)\n   - Identify any patterns in failures (specific sites, URL patterns, etc.)\n   - Provide recommendations for further improvements",
        "status": "pending",
        "dependencies": [
          5,
          10,
          11,
          12,
          13
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 15,
        "title": "Implement Playwright Screenshot and Structured Data Extraction Endpoint",
        "description": "Create a new web crawler endpoint that uses Playwright to navigate to a page, take a full-page screenshot, and extract structured data fields using the Ollama qwen2.5vl:7b model.",
        "details": "This task involves implementing a new endpoint in the web crawler service that combines visual and textual analysis:\n\n1. Create a new endpoint `/api/v1/extract-structured-data`:\n   - Accept URL parameter and optional fields to extract (default: name, price, currency, availability)\n   - Implement request validation and error handling\n\n2. Extend the WebCrawlerClient to support Playwright integration:\n   ```python\n   class WebCrawlerClient:\n       # Existing methods...\n       \n       async def crawl_with_screenshot(self, url: str) -> tuple[str, bytes]:\n           \"\"\"Navigate to URL and return both HTML content and full-page screenshot\"\"\"\n           async with async_playwright() as p:\n               browser = await p.chromium.launch()\n               page = await browser.new_page()\n               await page.goto(url, wait_until=\"networkidle\")\n               html_content = await page.content()\n               screenshot = await page.screenshot(full_page=True)\n               await browser.close()\n               return html_content, screenshot\n   ```\n\n3. Create a new StructuredDataExtractor class that uses the Ollama client:\n   ```python\n   class StructuredDataExtractor:\n       def __init__(self, ollama_client, model_name=\"qwen2.5vl:7b\"):\n           self.ollama_client = ollama_client\n           self.model_name = model_name\n           \n       async def extract_fields(self, html_content: str, screenshot: bytes, \n                               fields: list[str]) -> dict:\n           \"\"\"Extract structured data from HTML and screenshot using Ollama VLM\"\"\"\n           # Convert screenshot to base64\n           screenshot_b64 = base64.b64encode(screenshot).decode('utf-8')\n           \n           # Craft prompt for the VLM\n           prompt = f\"\"\"Extract the following fields from this product page: {', '.join(fields)}\n           Return ONLY a valid JSON object with these fields as keys.\n           If a field cannot be found, set its value to null.\"\"\"\n           \n           # Call Ollama with both text and image\n           response = await self.ollama_client.generate_with_context(\n               model=self.model_name,\n               prompt=prompt,\n               images=[screenshot_b64],\n               context={\"html\": html_content[:10000]}  # Truncate HTML if needed\n           )\n           \n           # Parse JSON response\n           try:\n               return json.loads(response.text)\n           except json.JSONDecodeError:\n               # Fallback: attempt to extract JSON from text response\n               return self._extract_json_from_text(response.text)\n   ```\n\n4. Implement the new endpoint handler:\n   ```python\n   @router.get(\"/api/v1/extract-structured-data\")\n   async def extract_structured_data(\n       request: Request,\n       url: str = Query(..., description=\"URL to extract data from\"),\n       fields: str = Query(\"name,price,currency,availability\", \n                          description=\"Comma-separated fields to extract\")\n   ):\n       try:\n           # Get shared clients from request context\n           web_crawler_client = request.app.state.web_crawler_client\n           ollama_client = request.app.state.ollama_client\n           logger = request.app.state.logger\n           \n           # Parse fields\n           field_list = [f.strip() for f in fields.split(\",\")]\n           \n           # Crawl page with screenshot\n           logger.info(f\"Crawling {url} with screenshot for structured data extraction\")\n           html_content, screenshot = await web_crawler_client.crawl_with_screenshot(url)\n           \n           # Extract structured data\n           extractor = StructuredDataExtractor(\n               ollama_client, \n               model_name=os.environ.get(\"OLLAMA_MODEL\", \"qwen2.5vl:7b\")\n           )\n           structured_data = await extractor.extract_fields(\n               html_content, screenshot, field_list\n           )\n           \n           return {\n               \"url\": url,\n               \"extracted_data\": structured_data,\n               \"fields_requested\": field_list\n           }\n       except Exception as e:\n           logger.error(f\"Error extracting structured data: {str(e)}\")\n           raise HTTPException(\n               status_code=500, \n               detail=f\"Failed to extract structured data: {str(e)}\"\n           )\n   ```\n\n5. Update Dockerfile to install Playwright and Chromium:\n   ```dockerfile\n   # Install Playwright and browsers\n   RUN pip install playwright\n   RUN playwright install chromium\n   RUN playwright install-deps chromium\n   ```\n\n6. Add environment variables to deployment configuration:\n   - OLLAMA_BASE_URL: URL of the Ollama server (default: http://ollama-service:11434)\n   - OLLAMA_MODEL: Model to use for extraction (default: qwen2.5vl:7b)\n\n7. Update README with endpoint documentation and environment variable descriptions",
        "testStrategy": "1. Unit Testing:\n   - Create unit tests for the StructuredDataExtractor class with mocked Ollama responses\n   - Test JSON parsing logic with various response formats\n   - Test error handling for malformed responses\n   - Test field filtering functionality\n\n2. Integration Testing:\n   - Set up a test environment with a local Ollama server running qwen2.5vl:7b\n   - Create integration tests that call the endpoint with real URLs\n   - Verify screenshot capture functionality works correctly\n   - Test with various e-commerce product pages to ensure field extraction accuracy\n   - Measure response times and optimize if necessary\n\n3. Edge Case Testing:\n   - Test with pages that load content dynamically via JavaScript\n   - Test with pages that have unusual layouts or structures\n   - Test with non-product pages to verify graceful handling\n   - Test with very large pages to ensure performance\n\n4. Environment Variable Testing:\n   - Verify the endpoint correctly uses OLLAMA_BASE_URL and OLLAMA_MODEL\n   - Test fallback behavior when environment variables are not set\n   - Test with different Ollama models to compare extraction quality\n\n5. Docker Build Verification:\n   - Build the Docker image and verify Playwright and Chromium are correctly installed\n   - Test the containerized application to ensure screenshot functionality works\n   - Verify browser dependencies are correctly installed\n\n6. Documentation Testing:\n   - Verify README accurately describes the new endpoint\n   - Test example API calls provided in documentation\n   - Ensure environment variable documentation is clear and complete",
        "status": "pending",
        "dependencies": [
          1,
          2,
          7
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Extend WebCrawlerClient with Playwright Integration",
            "description": "Implement the crawl_with_screenshot method in the WebCrawlerClient class to navigate to a URL and capture both HTML content and a full-page screenshot using Playwright.",
            "dependencies": [],
            "details": "Add the async method crawl_with_screenshot to the WebCrawlerClient class that launches a Playwright browser, navigates to the specified URL, waits for network idle, captures the HTML content and a full-page screenshot, then closes the browser and returns both. Include error handling for navigation failures and timeouts.",
            "status": "pending",
            "testStrategy": "Create unit tests with mocked Playwright responses. Test error handling for navigation failures. Create integration tests with real URLs to verify screenshot capture works correctly."
          },
          {
            "id": 2,
            "title": "Create StructuredDataExtractor Class",
            "description": "Implement a new class that uses the Ollama client with the qwen2.5vl:7b model to extract structured data fields from HTML content and screenshots.",
            "dependencies": [
              "15.1"
            ],
            "details": "Create the StructuredDataExtractor class with methods to extract specified fields from HTML and screenshot data. Implement base64 encoding for the screenshot, craft appropriate prompts for the VLM, call the Ollama client with both text and image inputs, and parse the JSON response. Include fallback logic for handling malformed JSON responses.",
            "status": "pending",
            "testStrategy": "Unit test with mocked Ollama responses for various field combinations. Test JSON parsing with well-formed and malformed responses. Test the fallback extraction logic with different response formats."
          },
          {
            "id": 3,
            "title": "Implement Extract Structured Data Endpoint",
            "description": "Create a new API endpoint that accepts a URL and optional fields to extract, then uses the WebCrawlerClient and StructuredDataExtractor to return structured data.",
            "dependencies": [
              "15.1",
              "15.2"
            ],
            "details": "Implement the /api/v1/extract-structured-data endpoint handler that validates input parameters, calls the WebCrawlerClient to get HTML and screenshot data, instantiates the StructuredDataExtractor, extracts the requested fields, and returns a structured response. Include comprehensive error handling and logging.",
            "status": "pending",
            "testStrategy": "Test the endpoint with various valid and invalid input combinations. Verify error responses for invalid URLs and server errors. Test with different field combinations to ensure correct extraction."
          },
          {
            "id": 4,
            "title": "Update Dockerfile for Playwright Support",
            "description": "Modify the Dockerfile to install Playwright and the required Chromium browser dependencies.",
            "dependencies": [],
            "details": "Update the Dockerfile to include commands for installing Playwright via pip, installing the Chromium browser, and installing the necessary system dependencies for Chromium to run headlessly in a container environment.",
            "status": "pending",
            "testStrategy": "Build the Docker image and verify Playwright and Chromium are correctly installed. Test running a simple Playwright script inside the container to ensure the browser launches successfully."
          },
          {
            "id": 5,
            "title": "Configure Environment Variables and Deployment Settings",
            "description": "Add the necessary environment variables and update deployment configuration to support the new structured data extraction functionality.",
            "dependencies": [
              "15.3",
              "15.4"
            ],
            "details": "Add environment variable definitions for OLLAMA_BASE_URL and OLLAMA_MODEL with appropriate default values. Update deployment configuration files (docker-compose.yml, Kubernetes manifests, etc.) to include these variables. Ensure the service has appropriate resource allocations for running Playwright.",
            "status": "pending",
            "testStrategy": "Test deployment with different environment variable configurations. Verify the service correctly connects to the Ollama server. Test with different model specifications to ensure flexibility."
          },
          {
            "id": 6,
            "title": "Update Documentation and Create Usage Examples",
            "description": "Update the README and API documentation to include information about the new endpoint, required environment variables, and usage examples.",
            "dependencies": [
              "15.3",
              "15.5"
            ],
            "details": "Add comprehensive documentation for the new /api/v1/extract-structured-data endpoint, including parameter descriptions, response format, and error codes. Create usage examples showing how to extract different field combinations. Document the environment variables and their default values. Include information about the Ollama model requirements and limitations.",
            "status": "pending",
            "testStrategy": "Review documentation for accuracy and completeness. Test the provided examples to ensure they work as documented. Have team members review the documentation for clarity and usability."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-08-05T19:29:45.107Z",
      "updated": "2025-08-08T00:56:21.537Z",
      "description": "Tasks for master context"
    }
  }
}