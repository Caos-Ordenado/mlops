---
description: Web crawler agent configuration and usage guidelines
globs: 
alwaysApply: true
---
# Web Crawler Agent Rules

## Project Overview
The web crawler is a high-performance agent with memory-adaptive features and a RESTful API, deployed in the home server Kubernetes cluster.

## Key Features
- Asynchronous web crawling with aiohttp
- Memory-adaptive crawling with configurable thresholds
- Multiple storage backends (Redis, PostgreSQL)
- RESTful API with FastAPI and OpenAPI documentation
- Kubernetes deployment with automated scripts
- Configurable logging levels and cleanup intervals
- Headless browser support with viewport configuration
- Automatic data retention management

## Deployment Guidelines

### Automated Deployment
Use the deployment script for automated updates:
```bash
cd agents/web_crawler
./deploy.sh
```

The script handles:
- Building AMD64 Docker image
- Transferring to home server (internal-vpn-address)
- Importing to microk8s
- Applying K8s configurations
- Deployment verification

### Security and Configuration
The deployment uses:
- **Existing Secrets**: Reuses PostgreSQL and Redis secrets for credentials
- **ConfigMap**: Non-sensitive configuration from .env file

The web crawler automatically uses:
- PostgreSQL credentials from `postgres-secrets` in shared namespace
- Redis password from `redis-secret` in shared namespace

No additional secret setup is required as it uses existing infrastructure secrets.

### Access Points
- Main API: http://home.server/crawler/
- Swagger UI: http://home.server/crawler/docs
- ReDoc UI: http://home.server/crawler/redoc

### Kubernetes Configuration
- Namespace: shared
- Deployment name: web-crawler
- Service name: web-crawler
- Ingress: Configured with Traefik
- Path prefix: /crawler (stripped by middleware)

## Environment Variables

### Core Settings
```env
CRAWLER_MAX_PAGES=10000
CRAWLER_MAX_DEPTH=20
CRAWLER_TIMEOUT=180000
CRAWLER_MAX_TOTAL_TIME=300
CRAWLER_MAX_CONCURRENT_PAGES=30
CRAWLER_MEMORY_THRESHOLD=80.0
CRAWLER_USER_AGENT=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.6312.58 Safari/537.36
CRAWLER_RESPECT_ROBOTS=false
CRAWLER_DEBUG=false
CRAWLER_LOG_LEVEL=INFO
CRAWLER_CLEANUP_INTERVAL_HOURS=24
CRAWLER_DATA_RETENTION_DAYS=30
CRAWLER_ALLOWED_DOMAINS=ai.pydantic.dev,example.com
CRAWLER_EXCLUDE_PATTERNS=*.pdf,*.jpg,*.png,*.gif,*.zip,*.doc,*.docx,*.xls,*.xlsx,*.ppt,*.pptx
CRAWLER_HEADLESS=true
CRAWLER_VIEWPORT_HEIGHT=1080
CRAWLER_VIEWPORT_WIDTH=1920
```

### Storage Settings
```env
CRAWLER_STORAGE_POSTGRES=true
CRAWLER_STORAGE_REDIS=true
POSTGRES_HOST=postgres.shared.svc.cluster.local
POSTGRES_PORT=5432
POSTGRES_USER=admin
POSTGRES_DB=web_crawler
REDIS_HOST=redis.shared.svc.cluster.local
REDIS_PORT=6379
REDIS_DB=0
```

## API Usage

### Crawl Endpoint
POST /crawl
```json
{
  "urls": ["https://example.com"],
  "max_pages": 100,
  "max_depth": 3,
  "allowed_domains": ["example.com"],
  "exclude_patterns": ["*.pdf", "*.jpg", "*.png", "*.gif", "*.zip"],
  "respect_robots": true,
  "timeout": 30000,
  "max_total_time": 60,
  "max_concurrent_pages": 5,
  "debug": false,
  "headless": true,
  "viewport_width": 1920,
  "viewport_height": 1080
}
```

Response:
```json
{
  "results": [
    {
      "url": "https://example.com",
      "title": "Example Domain",
      "text": "...",
      "html": "...",
      "links": ["https://example.com/page1", "..."],
      "metadata": {
        "status_code": 200,
        "headers": {},
        "content_type": "text/html",
        "timestamp": 1743511218.223563
      }
    }
  ],
  "total_urls": 1,
  "crawled_urls": 1,
  "elapsed_time": 0.211,
  "memory_usage": {
    "current": 45.2,
    "peak": 52.8
  }
}
```

### Monitoring Commands
```bash
# Pod status
kubectl get pods -n shared -l app=web-crawler

# Logs
kubectl logs -n shared -l app=web-crawler --tail=100

# Deployment status
kubectl describe deployment -n shared web-crawler

# ConfigMap
kubectl get configmap -n shared web-crawler-config -o yaml
```

## File Structure
```
web_crawler/
├── src/
│   ├── core/           # Core crawler implementation
│   ├── api/            # FastAPI application
│   ├── main.py         # Entry point
│   └── config.py       # Configuration
├── deploy.sh           # Deployment script
├── start.sh           # Local development script
└── requirements.txt    # Dependencies
```

## Best Practices
1. Always use the deployment script for updates
2. Keep sensitive data in secrets, not in .env files or ConfigMaps
3. Monitor memory usage through debug logs
4. Use appropriate storage backend based on needs
5. Check logs after deployment for any issues
6. Respect the memory threshold settings
7. Use the health endpoint for monitoring
8. Configure appropriate cleanup intervals
9. Set proper log levels for different environments
10. Monitor storage backend health

## Troubleshooting
1. If pod fails to start, check:
   - Image build and transfer logs
   - Pod events and logs
   - Storage connection settings
   - ConfigMap values
2. If crawler performance degrades:
   - Monitor memory usage
   - Check concurrent pages setting
   - Verify storage backend health
   - Review cleanup job logs
3. For API issues:
   - Verify Traefik routing
   - Check pod logs for errors
   - Validate request format
   - Check storage connectivity

# Storage Connections

### PostgreSQL
PostgreSQL is available in the shared namespace:
- Inside cluster: `postgres.shared.svc.cluster.local:5432`
- Database: `web_crawler`
- User: `admin`

When using PostgreSQL storage, make sure to:
- Set `CRAWLER_STORAGE_POSTGRES=true`
- Use the correct service DNS name, not localhost
- Check database connectivity before crawling large sites
- Monitor table growth and cleanup job execution

### Redis
Redis is available in the shared namespace:
- Inside cluster: `redis.shared.svc.cluster.local:6379`

When using Redis storage, make sure to:
- Set `CRAWLER_STORAGE_REDIS=true`
- Use the correct service DNS name, not localhost
- Monitor Redis memory usage for large crawls
- Configure appropriate data retention periods 