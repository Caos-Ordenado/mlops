---
description: Web crawler agent configuration and usage guidelines
globs: 
alwaysApply: true
---
# Web Crawler Agent Rules

## Project Overview
The web crawler is a high-performance agent with memory-adaptive features and a RESTful API, deployed in the home server Kubernetes cluster.

## Features

- Asynchronous web crawling with aiohttp
- Memory-adaptive crawling
  - Monitors system memory usage during crawling
  - Automatically adjusts crawling speed and concurrency based on memory usage
  - Prevents memory exhaustion by pausing when threshold is reached
  - Configurable memory threshold (default: 80%)
  - Memory usage logging can be enabled/disabled independently of general logging
    - Controlled via `CRAWLER_DEBUG` environment variable
    - When enabled, logs memory usage at key points:
      - Crawler initialization
      - Before/after URL crawling
      - Before/after storage operations
      - At the start/completion of each depth level
      - During memory threshold checks
    - Memory logging is optimized to minimize overhead when disabled
    - Can be used alongside different logging levels for other components
- Storage backends
  - Redis for fast, in-memory storage
  - PostgreSQL for persistent storage
- RESTful API with FastAPI
  - OpenAPI/Swagger documentation
  - Request/response validation
  - Async request handling
  - Health check endpoint
  - CORS support

## To run locally

1. Create a virtual environment (recommended):
```bash
python -m venv .venv
source .venv/bin/activate  # On Unix/macOS
# or
.venv\Scripts\activate  # On Windows
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Set up environment variables:
```bash
cp .env.example .env
```
Then edit `.env` with your specific settings.

## Automated Deployment
Use the deployment script for automated updates:
```bash
cd agents/web_crawler
./deploy.sh
```

The script handles:
- Building AMD64 Docker image
- Transferring to home server (internal-vpn-address)
- Importing to microk8s
- Applying K8s configurations
- Deployment verification

### Security and Configuration
The deployment uses:
- **Existing Secrets**: Reuses PostgreSQL and Redis secrets for credentials
- **ConfigMap**: Non-sensitive configuration from .env file

The web crawler automatically uses:
- PostgreSQL credentials from `postgres-secrets` in shared namespace
- Redis password from `redis-secret` in shared namespace

No additional secret setup is required as it uses existing infrastructure secrets.

### Access Points
- Main API: http://home.server/crawler/
- Swagger UI: http://home.server/crawler/docs
- ReDoc UI: http://home.server/crawler/redoc

### Kubernetes Configuration
- Namespace: shared
- Deployment name: web-crawler
- Service name: web-crawler
- Ingress: Configured with Traefik
- Path prefix: /crawler (stripped by middleware)

### Monitoring Commands
```bash
# Pod status
kubectl get pods -n shared -l app=web-crawler

# Logs
kubectl logs -n shared -l app=web-crawler --tail=100

# Deployment status
kubectl describe deployment -n shared web-crawler

# ConfigMap
kubectl get configmap -n shared web-crawler-config -o yaml
```
## REST API
 - Check the openpi definition of the service

## Readme
 - More info on [README.md](mdc:agents/web_crawler/README.md)

## Best Practices
1. Always use the deployment script for updates
2. Keep sensitive data in secrets, not in .env files or ConfigMaps
3. Monitor memory usage through debug logs
4. Use appropriate storage backend based on needs
5. Check logs after deployment for any issues
6. Respect the memory threshold settings
7. Use the health endpoint for monitoring
8. Configure appropriate cleanup intervals
9. Set proper log levels for different environments
10. Monitor storage backend health

## Troubleshooting
1. If pod fails to start, check:
   - Image build and transfer logs
   - Pod events and logs
   - Storage connection settings
   - ConfigMap values
2. If crawler performance degrades:
   - Monitor memory usage
   - Check concurrent pages setting
   - Verify storage backend health
   - Review cleanup job logs
3. For API issues:
   - Verify Traefik routing
   - Check pod logs for errors
   - Validate request format
   - Check storage connectivity

## Output and Logging

The crawler provides detailed output with configurable logging levels:

1. Console Output:
   - Shows real-time progress of the crawl
   - Displays basic information about crawled pages
   - Shows any errors or issues during crawling

2. Log File (`crawler.log`):
   - Contains detailed logging information based on LOG_LEVEL
   - Includes timestamps for each operation
   - Shows full crawl results and memory usage when debug is enabled
   - Log file is cleaned on each server start
   - Available log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL

# Storage Connections

### PostgreSQL
PostgreSQL is available in the shared namespace:
- Inside cluster: `postgres.shared.svc.cluster.local:5432`
- Database: `web_crawler`
- User: `admin`

When using PostgreSQL storage, make sure to:
- Set `CRAWLER_STORAGE_POSTGRES=true`
- Use the correct service DNS name, not localhost
- Check database connectivity before crawling large sites
- Monitor table growth and cleanup job execution

### Redis
Redis is available in the shared namespace:
- Inside cluster: `redis.shared.svc.cluster.local:6379`

When using Redis storage, make sure to:
- Set `CRAWLER_STORAGE_REDIS=true`
- Use the correct service DNS name, not localhost
- Monitor Redis memory usage for large crawls
- Configure appropriate data retention periods 